#!/usr/bin/env python

import math, os, socket, time, subprocess, datetime, sys, getpass, getopt, yaml
import boto, boto.s3.connection
from boto.s3.lifecycle import Lifecycle, Expiration
import smtplib
from filechunkio import FileChunkIO
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
import mysql.connector

config = {}

def main(argv):

	try:                                
		opts, args = getopt.getopt(argv, "hi", ["help", "init"])
	except getopt.GetoptError:
		usage()
		sys.exit(2)

	for opt, arg in opts:
		if opt in ('-h', '--help'):
			usage()
			sys.exit()
		elif opt in ('-i', '--init'):
			new_config()
			sys.exit()

	check_config()
	if load_config():
		global config
		config = load_config()

	# connect to s3
	conn = connect()
	if not conn:
		log('Could not connect to S3')
		sys.exit()

	list_folders(conn)

def check_config():
	config_path = '/etc/backup/'
	config_file = 'backup.yml'

	if os.path.exists(config_path+config_file):
		with open (config_path+config_file) as f:
			config = yaml.load(f)
	else:
		config = {'s3' : {'access_key' : '', 'secret' : ''}, 'mysql' : { 'user' : '', 'password' : '' } }
		log('No S3 Credentials')
		sys.exit('No S3 Credentials found therefore cannot continue.')
	return config

def load_config():
	config_path = '/etc/backup/'
	config_file = 'backup.yml'

	if os.path.exists(config_path+config_file):
		with open (config_path+config_file) as f:
			config = yaml.load(f)
	else:
		return False
	return config

def save_config(config):
	config_path = '/etc/backup/'
	config_file = 'backup.yml'
	if not os.path.exists(config_path):
		os.makedirs(config_path)
	with open(config_path+config_file, 'w') as outfile:
		outfile.write( yaml.dump(config, default_flow_style=False) )
		os.chmod(config_path, 0700)
		os.chmod(config_path+config_file, 0600)

def list_folders(conn):
	hostname = socket.gethostname()
	bucket = conn.get_bucket('my-bucket')
	keys = bucket.list(prefix=hostname+'/')
	folders  = []
	for key in keys:
		folders.append(key.name.rsplit('/',1)[0])
	folders = list(set(folders))
	dates = []
	for folder in folders:
		dates.append(time.mktime(time.strptime(folder, hostname+"/%Y/%m/%d")))
	dates.sort()

	print 'Todays date is: '+time.strftime('%A %d %B %Y')+'\n'
	print 'The following backups are available:'
	for index, value in enumerate(dates):
		date = datetime.datetime.fromtimestamp(value)
		print	'\t['+str(index)+'] '+date.strftime('%A %d %B %Y')

	print ''
	restore = long(raw_input('Which backup number do you wish to restore?\n'))

	while not (restore < len(dates)):
		print 'Sorry, that is not a valid option'
		restore = long(raw_input('Which backup number do you wish to restore?\n'))

	print ''
	print 'Downloading backup to ./restored'

	date_time = datetime.datetime.fromtimestamp(dates[restore]).strftime('/%Y/%m/%d')
	dest_path = 'restored/'+hostname + '/' + date_time 

	if os.path.exists(dest_path):
		command = 'rm -rf '+dest_path
		process = subprocess.Popen(command, shell=True)
		process.communicate()
		# make the folder(s)!
		os.makedirs(dest_path)
	else:
		# make the folder(s)!
		os.makedirs(dest_path)

	keys = bucket.list(prefix=hostname+date_time)
	for key in keys:
		print key.name
		key.get_contents_to_filename('restored/'+str(key.name))

	print 'Files downloaded'

def usage():
	print 'Usage: restore [OPTION...]'
	print '\t -h, --help \t\t\t give this help list'
	print '\t -i, --init \t\t\t force overwriting of S3 and MysQL Credentials\n'

	print 'Mandatory or optional arguments to long options are also mandatory or optional\nfor any corresponding short options.\n'

	print 'Report bugs to alex@asdfx.us'

def test_s3_credentials(access_key, secret):
	conn = boto.connect_s3(
		aws_access_key_id = access_key,
		aws_secret_access_key = secret,
		host = 'vault.my-host.co.uk',
		is_secure=True,
		calling_format = boto.s3.connection.OrdinaryCallingFormat(),
	)
	try:
		bucket = conn.get_bucket('my-bucket')
	except boto.exception.S3ResponseError as err:
		return False
	return True

def check_for_aws_credentials():
	home = os.path.expanduser("~")
	if(os.path.isfile(home+'/.boto')):
		return True
	return False

def log(text, level='info'):
	import logging
	logging.basicConfig(filename='/var/log/backup/backup.log', filemode='a+', level=logging.INFO, format='%(asctime)s %(message)s', datefmt='%m/%d/%Y %I:%M:%S %p')
	if(level=='info'):
		logging.info(text)
	elif(level=='warning'):
		logging.warning(text)

#connect to S3
def connect():
	global config
	
	access_key = config['s3']['access_key']
	secret = config['s3']['secret']

	try:
		conn = boto.connect_s3(
			host = 'vault.my-host.co.uk',
			aws_access_key_id = access_key,
			aws_secret_access_key = secret,
			is_secure=True,
			calling_format = boto.s3.connection.OrdinaryCallingFormat()
		)
		return conn
	except:
		log('Failed to connect to S3')
		return False

if __name__ == '__main__':
	main(sys.argv[1:])
